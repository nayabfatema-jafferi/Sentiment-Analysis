{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d8ec1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09ca9eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Complaints</th>\n",
       "      <th>Product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Communication tactics</td>\n",
       "      <td>Debt collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cont'd attempts collect debt not owed</td>\n",
       "      <td>Debt collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Application, originator, mortgage broker</td>\n",
       "      <td>Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Other</td>\n",
       "      <td>Credit card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cont'd attempts collect debt not owed</td>\n",
       "      <td>Debt collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Communication tactics</td>\n",
       "      <td>Debt collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Managing the loan or lease</td>\n",
       "      <td>Consumer loan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Communication tactics</td>\n",
       "      <td>Debt collection</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Complaints          Product\n",
       "0                     Communication tactics  Debt collection\n",
       "1     Cont'd attempts collect debt not owed  Debt collection\n",
       "2  Application, originator, mortgage broker         Mortgage\n",
       "3                                     Other      Credit card\n",
       "4     Cont'd attempts collect debt not owed  Debt collection\n",
       "5                     Communication tactics  Debt collection\n",
       "6                Managing the loan or lease    Consumer loan\n",
       "7                     Communication tactics  Debt collection"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = pd.read_csv(\"Complaints_Tickets.csv\")\n",
    "file.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69cac2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Debt collection            7494\n",
       "Mortgage                   6612\n",
       "Credit reporting           5717\n",
       "Credit card                2830\n",
       "Bank account or service    2609\n",
       "Consumer loan              1314\n",
       "Student loan                785\n",
       "Payday loan                 348\n",
       "Money transfers             232\n",
       "Prepaid card                175\n",
       "Other financial service      40\n",
       "Name: Product, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file[\"Product\"].value_counts() #to check the number of unique categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bf7d0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28156 entries, 0 to 28155\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Complaints  28154 non-null  object\n",
      " 1   Product     28156 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 440.1+ KB\n"
     ]
    }
   ],
   "source": [
    "#1. Text Normalizations\n",
    "file.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "696e9046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'application, originator, mortgage broker'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#replacing null values with text\n",
    "file[\"Complaints\"].replace({np.nan: \"\"}, inplace = True)\n",
    "file[\"clean_complaints\"] = file[\"Complaints\"].apply(lambda x : x.lower())\n",
    "file[\"clean_complaints\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "106efc12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'application  originator  mortgage broker'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing punctuation marks\n",
    "file[\"clean_complaints\"] = file[\"clean_complaints\"].apply(lambda x: re.sub(\"[^A-Za-z0-9]\", \" \", x))\n",
    "file[\"clean_complaints\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1ecc1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing addtional blank spaces, and blank spaces at the begining or end of sentences if any\n",
    "file[\"clean_complaints\"] = file[\"clean_complaints\"].apply(lambda x: re.sub(r\"\\s+\" , \" \", x))\n",
    "file[\"clean_complaints\"] = file[\"clean_complaints\"].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c733522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'application originator mortgage broker'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file[\"clean_complaints\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "013b806b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['communication', 'tactics']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizing the words\n",
    "file[\"clean_complaints\"] = file[\"clean_complaints\"].apply(lambda x: word_tokenize(x))\n",
    "file[\"clean_complaints\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b158f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            [communication, tactics]\n",
       "1               [cont, attempts, collect, debt, owed]\n",
       "2         [application, originator, mortgage, broker]\n",
       "3                                                  []\n",
       "4               [cont, attempts, collect, debt, owed]\n",
       "                             ...                     \n",
       "28151           [cont, attempts, collect, debt, owed]\n",
       "28152                           [taking, loan, lease]\n",
       "28153    [loan, servicing, payments, escrow, account]\n",
       "28154                  [stop, charges, bank, account]\n",
       "28155                           [transaction, issues]\n",
       "Name: clean_complaints, Length: 28156, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing stopwrds\n",
    "stop_words = stopwords.words(\"english\")\n",
    "file[\"clean_complaints\"] = file[\"clean_complaints\"].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "file[\"clean_complaints\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62b3e11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizing the words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatize = WordNetLemmatizer()\n",
    "file[\"clean_complaints\"] = file[\"clean_complaints\"].apply(lambda x: [lemmatize.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e33a20c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          communication tactic\n",
       "1                cont attempt collect debt owed\n",
       "2        application originator mortgage broker\n",
       "3                                              \n",
       "4                cont attempt collect debt owed\n",
       "                          ...                  \n",
       "28151            cont attempt collect debt owed\n",
       "28152                         taking loan lease\n",
       "28153     loan servicing payment escrow account\n",
       "28154                  stop charge bank account\n",
       "28155                         transaction issue\n",
       "Name: clean_complaints, Length: 28156, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#joining it back\n",
    "file[\"clean_complaints\"] = file[\"clean_complaints\"].apply(lambda x: \" \".join(map(str,x)))\n",
    "file[\"clean_complaints\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fd4eaa",
   "metadata": {},
   "source": [
    "###### Text Vectorization using TFidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69008839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(file[\"clean_complaints\"], file[\"Product\"], test_size=0.15, random_state= 2)\n",
    "tfidf = TfidfVectorizer(use_idf=True)\n",
    "X_train_vec = tfidf.fit_transform(X_train)\n",
    "X_test_vec = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b41d8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TFIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>false</th>\n",
       "      <td>0.582563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>representation</th>\n",
       "      <td>0.582563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>statement</th>\n",
       "      <td>0.566782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>problem</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>owed</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   TFIDF\n",
       "false           0.582563\n",
       "representation  0.582563\n",
       "statement       0.566782\n",
       "problem         0.000000\n",
       "owed            0.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#printing tfidf result for 1st row\n",
    "pd.DataFrame(X_train_vec[0].T.todense(), index = tfidf.get_feature_names_out(),columns = [\"TFIDF\"]).sort_values(by = \"TFIDF\", ascending = False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acff2c67",
   "metadata": {},
   "source": [
    "##### Using Deep Learning for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3ede193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4107341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 1 : Tokenizing the clean_complaints\n",
    "max_tokens = 1000\n",
    "tokenizer = Tokenizer(num_words = max_tokens , filters= '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(file[\"clean_complaints\"].values)\n",
    "#getting the length of tokenized words\n",
    "len(tokenizer.word_index)  #hence there are only 158 unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "033f47f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28156, 15)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#paddind the sequneces to make it fit a standard length\n",
    "pad_len = 15\n",
    "padded_sent = tokenizer.texts_to_sequences(file[\"clean_complaints\"].values)\n",
    "X = pad_sequences(padded_sent, maxlen = pad_len)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "780f1969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28156, 11)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using pd.getdummies on file[\"Product\"]\n",
    "y = pd.get_dummies(file[\"Product\"])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4de8cdb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23932, 15) (23932, 11)\n"
     ]
    }
   ],
   "source": [
    "#splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=2)\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8510f3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 15, 64)            64000     \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 15, 64)           0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               98816     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 11)                1419      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 164,235\n",
      "Trainable params: 164,235\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#building the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_tokens, 64, input_length= X_train.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(128, dropout = 0.2, recurrent_dropout = 0.2))\n",
    "model.add(Dense(11, activation = \"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f08b1d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "keras_callbacks = EarlyStopping(monitor=\"val_loss\", min_delta = 0.01, patience = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae058fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "374/374 [==============================] - 27s 63ms/step - loss: 0.4021 - accuracy: 0.8891 - val_loss: 0.0428 - val_accuracy: 0.9905\n",
      "Epoch 2/4\n",
      "374/374 [==============================] - 22s 60ms/step - loss: 0.0414 - accuracy: 0.9894 - val_loss: 0.0296 - val_accuracy: 0.9927\n",
      "Epoch 3/4\n",
      "374/374 [==============================] - 22s 59ms/step - loss: 0.0322 - accuracy: 0.9898 - val_loss: 0.0247 - val_accuracy: 0.9931\n",
      "Epoch 4/4\n",
      "374/374 [==============================] - 23s 62ms/step - loss: 0.0296 - accuracy: 0.9908 - val_loss: 0.0229 - val_accuracy: 0.9915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16aef614880>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs = 4, batch_size = 64, callbacks=keras_callbacks, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f95214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
